# YOLOV8剪枝项目介绍

## 对于群里的剪枝相关问题,我基本都会回复,对于一些剪枝问题,我都会给出建议。  

### 首先剪枝是什么？  
模型剪枝是深度学习中的一种技术，旨在通过减少神经网络中不必要的参数和连接，来优化模型的效率和性能。模型剪枝可以分为结构剪枝和参数剪枝两种类型。  

### 为什么需要剪枝？  
剪枝可以很好地衡量模型轻量化程度与精度的关系,是替换轻量化结构完全没办法比的,比如我模型剪枝可以压缩百分之30的计算量,精度只下降了百分之1,但是你通过换模块来达到压缩百分之30的计算量,一般时间就会变长,因为大部分轻量化模块都是由时间换空间,而且精度还会下降得比较多,但是剪枝可以很好地避免这个问题.

### 目前剪枝项目包含以下剪枝方法：
1. L1 
2. Random 
3. Slim 
4. GroupSlim 
5. GroupNorm 
6. LAMP 
7. GroupSL 
8. GroupReg
9. GroupHessian
10. GroupTaylor

### 其中prune系列还有一些细节：
1. 支持稀疏训练时候可视化BN稀疏程度和数值。
2. 稀疏训练的稀疏系数会进行线性调整，让稀疏训练后期精度更容易回升，更稳定。
3. 支持设定加速比例，模型会进行自动压缩，压缩到指定比例或者达到最大压缩次数后会自动进入finetune。

### 剪枝的一些顾虑
大家关心最多的一个问题就是，我的结构能不能剪之类的，剪枝对模型复杂度的要求比较高，目前剪枝都是基于Torch_Pruning库进行剪枝，prune系列的可以跳过一些不能剪枝的层(某些复杂的结构可能在构建动态图的时候失败,这些就只能换结构)，这个项目会有比较多的示例和视频教程教大家如何去剪自己的结构,注意点在哪里等等。这个剪枝项目是没办法保证所有的结构都能剪，有一定的风险，是否入手请自行考虑！  
[yolov5v7剪枝](https://github.com/z1069614715/objectdetection_script/blob/master/yolo-improve/yolov5v7-light.md)这里面的结构都经过实验是可剪的.

### 那些人群建议入手剪枝
1. 原始的算法精度很高,没办法再提升精度,只能走轻量化路线,这种建议配合一些轻量化模块+剪枝来增加你的工作量和创新度.
2. 需要部署到嵌入式或者手机端等低算力设备,这类本身模型就不能太复杂,而且以轻量化为主,剪枝是非常适合的.
3. 以后需从事深度学习方面的工作,模型轻量化(蒸馏、量化、剪枝)基本是必须要会的技能.

### Yolov8 相关实验 GPU-Device:RTX3090
#### Dataset:VisDrone 30%TrainingData Model:Yolov8n
| model | Parameters | GFLOPs | Model Size | mAP50 | mAP50-95 | Inference Time(bs:32) |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| BaseLine | 3,007,598 | 8.1 | 5.9m | 0.225 | 0.124 | 0.00099s |
| Lamp Exp1 2.0X | 1,513,245(50.3%) | 4.0(50%) | 3.1m(52.5%) | 0.197(-0.018) | 0.106(-0.018) | 0.00075s(75.8%) |
| Lamp Exp2 2.0X | 679,484(22.6%) | 4.0(50%) | 1.5m(25.4%) | 0.231(+0.006) | 0.126(+0.002) | 0.00073s(73.7%) |
| Lamp Exp3 2.5X | 503,959(16.8%) | 3.2(39.5%) | 1.2m(20.3%) | 0.225(0.0) | 0.123(-0.001) | 0.00068s(68.7%) |
| Group-Taylor Exp1 2.0X | 1,093,305(36.4%) | 4.0(50%) | 2.3m(39%) | 0.203(-0.022) | 0.11(-0.014) | 0.00074s(74.8%) |
| Group-Taylor Exp2 2.0X | 1,513,245(50.3%) | 4.0(50%) | 3.1m(52.5%) | 0.196(-0.029) | 0.105(-0.019) | 0.00075s(75.8%) |
| Group-Hessian Exp1 2.0X | 1,436,390(47.8%) | 4.0(50%) | 3.0m(50.8%) | 0.168(-0.057) | 0.0883(-0.041) | 0.00071s(71.7%) |
| Group-Sl Exp1 2.0X | 1,556,422(51.7%) | 4.0(50%) | 3.1m(52.5%) | 0.173(-0.052) | 0.0901(-0.0339) | 0.00066s(66.7%) |
| Group-Slim Exp1 2.0X | 1,113,000(37%) | 4.0(50%) | 2.3m(39%) | 0.201(-0.024) | 0.108(-0.016) | 0.00075s(75.8%) |
| Slim Exp1 2.0X | 932,902(31%) | 4.0(50%) | 2.0m(33.9%) | 0.21(-0.015) | 0.114(-0.01) | 0.00075s(75.8%) |

#### Dataset:VisDrone 30%TrainingData Model:yolov8-Faster-GFPN-P2-EfficientHead
| model | Parameters | GFLOPs | Model Size | mAP50 | mAP50-95 | Inference Time(bs:32) |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| BaseLine | 3,457,400 | 12.1 | 7.2M | 0.241 | 0.133 | 0.00188s |
| Lamp Exp1 2.0X | 903,894(26.1%) | 5.9(48.6%) | 2.3M(32%) | 0.226(-0.015) | 0.127(-0.006) | 0.00150s(83.3%) |
| GroupTaylor Exp1 2.0X | 1,699,046(49.1%) | 5.9(48.6%) | 3.9M(54.2%) | 0.212(-0.029) | 0.115(-0.028) | 0.00142s(75.5%) |
| GroupTaylor Exp2 2.0X | 1,751,941(51%) | 6.0(49.6%) | 4.0M(55.6%) | 0.216(-0.025) | 0.119(-0.024) | 0.00147s(78.2%) |
| GroupHessian Exp1 2.0X | 1,751,941(51%) | 6.0(49.6%) | 2.3M(32%) | 0.214(-0.023) | 0.118(-0.025) | 0.00147s(78.2%) |

#### Dataset:Seaship BaseLine:Yolov8n Light:yolov8-BIFPN-EfficientRepHead.yaml(C2f-EMBC,BIFPN,EfficientRepHead)
| model | Parameters | GFLOPs | Model Size | mAP50 | mAP50-95 | Inference Time(bs:32) |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| BaseLine | 3,006,818 | 8.1 | 5.9M | 0.986 | 0.813 | 0.00098s |
| Light | 1,809,166(60.2%) | 5.6(69.1%) | 4.5M(76.3%) | 0.981(-0.005) | 0.787(-0.026) | 0.00109s(112.2%) |
| Light Lamp Exp1 2.0X | 729,717(24.3%) | 2.4(30%) | 2.3M(39%) | 0.981(-0.005) | 0.777(-0.036) | 0.00080s(81.6%) |
| Light Lamp Exp2 2.5X | 492,731(16.4%) | 1.6(19.8%) | 1.8M(31%) | 0.973(-0.013) | 0.746(-0.067) | 0.00062s(63.3%) |

#### Dataset:VisDrone 100%TrainingData Model:yolov8-ASF-P2
| model | Parameters | GFLOPs | Model Size | mAP50 | mAP50-95 | Inference Time(bs:32) |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| BaseLine | 2,490,488 | 12.0 | 5.0M | 0.295 | 0.166 | 0.00199s |
| Lamp Exp1 2.0X | 664,162(26.7%) | 5.9(49.2%) | 2.3M(46%) | 0.277(-0.018) | 0.154(-0.012) | 0.00153s(76.9%) |
| Lamp Exp2 1.5X | 1,065,363(42.8%) | 7.9(65.8%) | 2.4M(48%) | 0.296(+0.001) | 0.165(-0.001) | 0.00168s(84.4%) |
| Lamp Exp3 1.7X | 885,911(35.6%) | 7.0(58.3%) | 2.3M(46%) | 0.29(-0.005) | 0.161(-0.005) | 0.00162s(81.4%) |

#### Dataset:VisDrone 30%TrainingData Model:yolov8-GHostHGNetV2-SlimNeck-ASF
| model | Parameters | GFLOPs | Model Size | mAP50 | mAP50-95 | Inference Time(bs:32) |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| BaseLine | 2,236,610 | 6.8 | 4.6M | 0.206 | 0.111 | 0.00137s |
| LAMP Exp1 2.0X | 951,571(42.5%) | 3.4(50%) | 2.1M(45.7%) | 0.207(+0.001) | 0.112(+0.001) | 0.00092s(67.2%) |

#### Dataset:CrowdHuman 20%TrainingData Model:yolov8-convnextv2-goldyolo-ASF
| model | Parameters | GFLOPs | Model Size | mAP50 | mAP50-95 | Inference Time(bs:32) |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| BaseLine | 8,712,945 | 16.7 | 17.0M | 0.747 | 0.431 | 0.00461s |
| LAMP Exp1 2.0X | 4,493,135(51.6%) | 8.3(49.7%) | 9.0M(52.9%) | 0.747(0.0) | 0.434(+0.003) | 0.00261s(56.6%) |
| LAMP Exp2 2.5X | 3,899,980(44.8%) | 6.6(39.5%) | 7.9M(46.5%) | 0.742(-0.005) | 0.431(0.0) | 0.00219s(47.5%) |